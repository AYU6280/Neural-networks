{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTmLyqrFJzUa882JXmNNY2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bPgxd7bxcR_",
        "outputId": "53a00841-327a-464c-c8c2-28bf281eb75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "words = list(wn.words())\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx_x1qJ015JV",
        "outputId": "492457d3-b2c7-49e8-abf9-f944b337a72f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.22-caliber',\n",
              " '.22-calibre',\n",
              " '.22_caliber',\n",
              " '.22_calibre',\n",
              " '.38-caliber',\n",
              " '.38-calibre',\n",
              " '.38_caliber',\n",
              " '.38_calibre',\n",
              " '.45-caliber',\n",
              " '.45-calibre',\n",
              " '.45_caliber',\n",
              " '.45_calibre',\n",
              " '0',\n",
              " '1',\n",
              " '10',\n",
              " '10-membered',\n",
              " '100',\n",
              " '1000',\n",
              " '1000th',\n",
              " '100th',\n",
              " '101',\n",
              " '101st',\n",
              " '105',\n",
              " '105th',\n",
              " '10th',\n",
              " '11',\n",
              " '110',\n",
              " '110th',\n",
              " '115',\n",
              " '115th',\n",
              " '11th',\n",
              " '12',\n",
              " '120',\n",
              " '120th',\n",
              " '125',\n",
              " '125th',\n",
              " '12th',\n",
              " '13',\n",
              " '130',\n",
              " '130th',\n",
              " '135',\n",
              " '135th',\n",
              " '13th',\n",
              " '14',\n",
              " '140',\n",
              " '140th',\n",
              " '145',\n",
              " '145th',\n",
              " '14th',\n",
              " '15',\n",
              " '150',\n",
              " '150th',\n",
              " '155',\n",
              " '155th',\n",
              " '15th',\n",
              " '16',\n",
              " '160',\n",
              " '160th',\n",
              " '165',\n",
              " '165th',\n",
              " '16th',\n",
              " '17',\n",
              " '170',\n",
              " '170th',\n",
              " '175',\n",
              " '175th',\n",
              " '17th',\n",
              " '18',\n",
              " '180',\n",
              " '180th',\n",
              " '18th',\n",
              " '19',\n",
              " '190',\n",
              " '190th',\n",
              " '19th',\n",
              " '1st',\n",
              " '2',\n",
              " '2-dimensional',\n",
              " '20',\n",
              " '200',\n",
              " '200th',\n",
              " '20th',\n",
              " '21',\n",
              " '21st',\n",
              " '22',\n",
              " '22nd',\n",
              " '23',\n",
              " '23rd',\n",
              " '24',\n",
              " '24th',\n",
              " '25',\n",
              " '25th',\n",
              " '26',\n",
              " '26th',\n",
              " '27',\n",
              " '27th',\n",
              " '28',\n",
              " '28th',\n",
              " '29',\n",
              " '29th',\n",
              " '2d',\n",
              " '2nd',\n",
              " '3',\n",
              " '3-dimensional',\n",
              " '3-membered',\n",
              " '30',\n",
              " '300',\n",
              " '300th',\n",
              " '30th',\n",
              " '31',\n",
              " '31st',\n",
              " '32',\n",
              " '32nd',\n",
              " '33',\n",
              " '33rd',\n",
              " '34',\n",
              " '34th',\n",
              " '35',\n",
              " '35th',\n",
              " '36',\n",
              " '36th',\n",
              " '37',\n",
              " '37th',\n",
              " '38',\n",
              " '38th',\n",
              " '39',\n",
              " '39th',\n",
              " '3rd',\n",
              " '4',\n",
              " '4-dimensional',\n",
              " '4-membered',\n",
              " '40',\n",
              " '400',\n",
              " '400th',\n",
              " '40th',\n",
              " '41',\n",
              " '41st',\n",
              " '42',\n",
              " '42nd',\n",
              " '43',\n",
              " '43rd',\n",
              " '44',\n",
              " '44th',\n",
              " '45',\n",
              " '45th',\n",
              " '46',\n",
              " '46th',\n",
              " '47',\n",
              " '47th',\n",
              " '48',\n",
              " '48th',\n",
              " '49',\n",
              " '49th',\n",
              " '4th',\n",
              " '5',\n",
              " '5-membered',\n",
              " '50',\n",
              " '500',\n",
              " '500th',\n",
              " '50th',\n",
              " '51',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '55',\n",
              " '55th',\n",
              " '56',\n",
              " '57',\n",
              " '58',\n",
              " '59',\n",
              " '5th',\n",
              " '6',\n",
              " '6-membered',\n",
              " '60',\n",
              " '60th',\n",
              " '61',\n",
              " '62',\n",
              " '63',\n",
              " '64',\n",
              " '64th',\n",
              " '65',\n",
              " '65th',\n",
              " '66',\n",
              " '67',\n",
              " '68',\n",
              " '69',\n",
              " '6th',\n",
              " '7',\n",
              " '7-membered',\n",
              " '70',\n",
              " '70th',\n",
              " '71',\n",
              " '72',\n",
              " '73',\n",
              " '74',\n",
              " '75',\n",
              " '75th',\n",
              " '76',\n",
              " '77',\n",
              " '78',\n",
              " '79',\n",
              " '7th',\n",
              " '8',\n",
              " '8-membered',\n",
              " '80',\n",
              " '80th',\n",
              " '81',\n",
              " '82',\n",
              " '83',\n",
              " '84',\n",
              " '85',\n",
              " '85th',\n",
              " '86',\n",
              " '87',\n",
              " '88',\n",
              " '89',\n",
              " '8th',\n",
              " '9',\n",
              " '9-membered',\n",
              " '90',\n",
              " '90th',\n",
              " '91',\n",
              " '92',\n",
              " '93',\n",
              " '94',\n",
              " '95',\n",
              " '95th',\n",
              " '96',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '9th',\n",
              " 'a-ok',\n",
              " 'a-okay',\n",
              " 'a-one',\n",
              " 'a.m.',\n",
              " 'a_cappella',\n",
              " 'a_couple_of',\n",
              " 'a_few',\n",
              " 'a_la_carte',\n",
              " 'a_la_mode',\n",
              " 'a_posteriori',\n",
              " 'a_priori',\n",
              " 'abactinal',\n",
              " 'abandoned',\n",
              " 'abashed',\n",
              " 'abasic',\n",
              " 'abatable',\n",
              " 'abatic',\n",
              " 'abaxial',\n",
              " 'abbatial',\n",
              " 'abbreviated',\n",
              " 'abdicable',\n",
              " 'abdominal',\n",
              " 'abdominous',\n",
              " 'abdominovesical',\n",
              " 'abducent',\n",
              " 'abducting',\n",
              " 'abecedarian',\n",
              " 'aberdonian',\n",
              " 'aberrant',\n",
              " 'abeyant',\n",
              " 'abhorrent',\n",
              " 'abiding',\n",
              " 'abient',\n",
              " 'abiogenetic',\n",
              " 'abject',\n",
              " 'abkhaz',\n",
              " 'abkhazian',\n",
              " 'ablated',\n",
              " 'ablative',\n",
              " 'ablaze',\n",
              " 'able',\n",
              " 'able-bodied',\n",
              " 'abloom',\n",
              " 'ablutionary',\n",
              " 'abnaki',\n",
              " 'abnormal',\n",
              " 'abolishable',\n",
              " 'abolitionary',\n",
              " 'abomasal',\n",
              " 'abominable',\n",
              " 'aboral',\n",
              " 'aboriginal',\n",
              " 'abortifacient',\n",
              " 'abortive',\n",
              " 'aboulic',\n",
              " 'abounding',\n",
              " 'about',\n",
              " 'above',\n",
              " 'above-mentioned',\n",
              " 'above-named',\n",
              " 'aboveboard',\n",
              " 'aboveground',\n",
              " 'abranchial',\n",
              " 'abranchiate',\n",
              " 'abranchious',\n",
              " 'abrasive',\n",
              " 'abreast',\n",
              " 'abridged',\n",
              " 'abroach',\n",
              " 'abroad',\n",
              " 'abrupt',\n",
              " 'abruptly-pinnate',\n",
              " 'abscessed',\n",
              " 'absent',\n",
              " 'absentminded',\n",
              " 'absolute',\n",
              " 'absolutist',\n",
              " 'absolutistic',\n",
              " 'absolved',\n",
              " 'absolvitory',\n",
              " 'absorbable',\n",
              " 'absorbed',\n",
              " 'absorbefacient',\n",
              " 'absorbent',\n",
              " 'absorbing',\n",
              " 'absorptive',\n",
              " 'abstemious',\n",
              " 'abstentious',\n",
              " 'abstinent',\n",
              " 'abstract',\n",
              " 'abstracted',\n",
              " 'abstractionist',\n",
              " 'abstractive',\n",
              " 'abstruse',\n",
              " 'absurd',\n",
              " 'abulic',\n",
              " 'abundant',\n",
              " 'abused',\n",
              " 'abusive',\n",
              " 'abuzz',\n",
              " 'abysmal',\n",
              " 'abyssal',\n",
              " 'academic',\n",
              " 'acanthoid',\n",
              " 'acanthotic',\n",
              " 'acanthous',\n",
              " 'acapnial',\n",
              " 'acapnic',\n",
              " 'acapnotic',\n",
              " 'acarpellous',\n",
              " 'acarpelous',\n",
              " 'acarpous',\n",
              " 'acatalectic',\n",
              " 'acaudal',\n",
              " 'acaudate',\n",
              " 'acaulescent',\n",
              " 'accelerando',\n",
              " 'accelerated',\n",
              " 'accelerative',\n",
              " 'acceleratory',\n",
              " 'accented',\n",
              " 'accentual',\n",
              " 'acceptable',\n",
              " 'acceptant',\n",
              " 'accepted',\n",
              " 'accepting',\n",
              " 'acceptive',\n",
              " 'accessary',\n",
              " 'accessible',\n",
              " 'accessional',\n",
              " 'accessorial',\n",
              " 'accessory',\n",
              " 'accident-prone',\n",
              " 'accidental',\n",
              " 'accipitrine',\n",
              " 'acclivitous',\n",
              " 'accommodating',\n",
              " 'accommodational',\n",
              " 'accommodative',\n",
              " 'accompanied',\n",
              " 'accompanying',\n",
              " 'accomplishable',\n",
              " 'accomplished',\n",
              " 'accordant',\n",
              " 'according',\n",
              " 'accountable',\n",
              " 'accoutered',\n",
              " 'accoutred',\n",
              " 'accredited',\n",
              " 'accretionary',\n",
              " 'accretive',\n",
              " 'accrued',\n",
              " 'acculturational',\n",
              " 'acculturative',\n",
              " 'accumbent',\n",
              " 'accumulated',\n",
              " 'accumulative',\n",
              " 'accurate',\n",
              " 'accursed',\n",
              " 'accurst',\n",
              " 'accusative',\n",
              " 'accusatorial',\n",
              " 'accusatory',\n",
              " 'accusing',\n",
              " 'accusive',\n",
              " 'accustomed',\n",
              " 'ace',\n",
              " 'acellular',\n",
              " 'acentric',\n",
              " 'acephalous',\n",
              " 'acerate',\n",
              " 'acerb',\n",
              " 'acerbic',\n",
              " 'acerose',\n",
              " 'acervate',\n",
              " 'acetabular',\n",
              " 'acetic',\n",
              " 'acetonic',\n",
              " 'acetose',\n",
              " 'acetous',\n",
              " 'acetylenic',\n",
              " 'acetylic',\n",
              " 'achaean',\n",
              " 'achenial',\n",
              " 'acheronian',\n",
              " 'acherontic',\n",
              " 'achievable',\n",
              " 'aching',\n",
              " 'achlamydeous',\n",
              " 'achlorhydric',\n",
              " 'achondritic',\n",
              " 'achondroplastic',\n",
              " 'achromatic',\n",
              " 'achromatinic',\n",
              " 'achromatous',\n",
              " 'achromic',\n",
              " 'achromous',\n",
              " 'achy',\n",
              " 'acicular',\n",
              " 'aciculate',\n",
              " 'acid',\n",
              " 'acid-fast',\n",
              " 'acid-forming',\n",
              " 'acid-loving',\n",
              " 'acid-tasting',\n",
              " 'acidic',\n",
              " 'acidimetric',\n",
              " 'acidophilic',\n",
              " 'acidophilous',\n",
              " 'acidotic',\n",
              " 'acidulent',\n",
              " 'acidulous',\n",
              " 'aciduric',\n",
              " 'acinar',\n",
              " 'acinic',\n",
              " 'acinose',\n",
              " 'acinous',\n",
              " 'acknowledgeable',\n",
              " 'acknowledged',\n",
              " 'acned',\n",
              " 'acneiform',\n",
              " 'acold',\n",
              " 'acorn-shaped',\n",
              " 'acoustic',\n",
              " 'acoustical',\n",
              " 'acquainted',\n",
              " 'acquiescent',\n",
              " 'acquirable',\n",
              " 'acquired',\n",
              " 'acquisitive',\n",
              " 'acquitted',\n",
              " 'acrid',\n",
              " 'acrimonious',\n",
              " 'acritical',\n",
              " 'acrobatic',\n",
              " 'acrocarpous',\n",
              " 'acrocentric',\n",
              " 'acrogenic',\n",
              " 'acrogenous',\n",
              " 'acromegalic',\n",
              " 'acronymic',\n",
              " 'acronymous',\n",
              " 'acropetal',\n",
              " 'acrophobic',\n",
              " 'acroscopic',\n",
              " 'across-the-board',\n",
              " 'actable',\n",
              " 'actinal',\n",
              " 'acting',\n",
              " 'actinic',\n",
              " 'actinoid',\n",
              " 'actinometric',\n",
              " 'actinometrical',\n",
              " 'actinomorphic',\n",
              " 'actinomorphous',\n",
              " 'actinomycetal',\n",
              " 'actinomycetous',\n",
              " 'actinomycotic',\n",
              " 'actionable',\n",
              " 'activated',\n",
              " 'activating',\n",
              " 'active',\n",
              " 'activist',\n",
              " 'activistic',\n",
              " 'actual',\n",
              " 'actuarial',\n",
              " 'actuated',\n",
              " 'actuating',\n",
              " 'acuate',\n",
              " 'aculeate',\n",
              " 'aculeated',\n",
              " 'acuminate',\n",
              " 'acute',\n",
              " 'acyclic',\n",
              " 'ad-lib',\n",
              " 'ad_hoc',\n",
              " 'ad_hominem',\n",
              " 'adactylous',\n",
              " 'adagio',\n",
              " 'adamant',\n",
              " 'adamantine',\n",
              " 'adaptable',\n",
              " 'adaptational',\n",
              " 'adaptative',\n",
              " 'adapted',\n",
              " 'adaptive',\n",
              " 'adaxial',\n",
              " 'addable',\n",
              " 'addible',\n",
              " 'addicted',\n",
              " 'addictive',\n",
              " 'additional',\n",
              " 'additive',\n",
              " 'addlebrained',\n",
              " 'addled',\n",
              " 'addlepated',\n",
              " 'addressable',\n",
              " 'addressed',\n",
              " 'adducent',\n",
              " 'adducting',\n",
              " 'adductive',\n",
              " 'adenocarcinomatous',\n",
              " 'adenoid',\n",
              " 'adenoidal',\n",
              " 'adept',\n",
              " 'adequate',\n",
              " 'adequate_to',\n",
              " 'adherent',\n",
              " 'adhesive',\n",
              " 'adiabatic',\n",
              " 'adient',\n",
              " 'adipose',\n",
              " 'adjacent',\n",
              " 'adjectival',\n",
              " 'adjective',\n",
              " 'adjudicative',\n",
              " 'adjudicatory',\n",
              " 'adjunct',\n",
              " 'adjunctive',\n",
              " 'adjuratory',\n",
              " 'adjustable',\n",
              " 'adjusted',\n",
              " 'adjustive',\n",
              " 'adjuvant',\n",
              " 'administrable',\n",
              " 'administrative',\n",
              " 'admirable',\n",
              " 'admired',\n",
              " 'admissible',\n",
              " 'admissive',\n",
              " 'admittable',\n",
              " 'admittible',\n",
              " 'admonishing',\n",
              " 'admonitory',\n",
              " 'adnate',\n",
              " 'adnexal',\n",
              " 'adolescent',\n",
              " 'adonic',\n",
              " 'adoptable',\n",
              " 'adopted',\n",
              " 'adoptive',\n",
              " 'adorable',\n",
              " 'adored',\n",
              " 'adoring',\n",
              " 'adorned',\n",
              " 'adpressed',\n",
              " 'adrenal',\n",
              " 'adrenergic',\n",
              " 'adrenocortical',\n",
              " 'adrenocorticotrophic',\n",
              " 'adrenocorticotropic',\n",
              " 'adrift',\n",
              " 'adroit',\n",
              " 'adscititious',\n",
              " 'adscript',\n",
              " 'adscripted',\n",
              " 'adsorbable',\n",
              " 'adsorbate',\n",
              " 'adsorbent',\n",
              " 'adsorptive',\n",
              " 'adulatory',\n",
              " 'adult',\n",
              " 'adulterant',\n",
              " 'adulterate',\n",
              " 'adulterated',\n",
              " 'adulterating',\n",
              " 'adulterine',\n",
              " 'adulterous',\n",
              " 'adumbrative',\n",
              " 'adust',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advancing',\n",
              " 'advantageous',\n",
              " 'advective',\n",
              " 'adventitial',\n",
              " 'adventitious',\n",
              " 'adventive',\n",
              " 'adventuresome',\n",
              " 'adventuristic',\n",
              " 'adventurous',\n",
              " 'adverbial',\n",
              " 'adversative',\n",
              " 'adverse',\n",
              " 'advertent',\n",
              " 'advertised',\n",
              " 'advisable',\n",
              " 'advised',\n",
              " 'advisory',\n",
              " 'adynamic',\n",
              " 'aecial',\n",
              " 'aegean',\n",
              " 'aeolian',\n",
              " 'aeolotropic',\n",
              " 'aeonian',\n",
              " 'aerated',\n",
              " 'aerial',\n",
              " 'aeriferous',\n",
              " 'aeriform',\n",
              " 'aerobic',\n",
              " 'aerobiotic',\n",
              " 'aerodynamic',\n",
              " 'aerolitic',\n",
              " 'aerological',\n",
              " 'aeromechanic',\n",
              " 'aeromedical',\n",
              " 'aeronautic',\n",
              " 'aeronautical',\n",
              " 'aerophilatelic',\n",
              " 'aerophilic',\n",
              " 'aerophilous',\n",
              " 'aerosolised',\n",
              " 'aerosolized',\n",
              " 'aery',\n",
              " 'aeschylean',\n",
              " 'aesculapian',\n",
              " 'aesthetic',\n",
              " 'aesthetical',\n",
              " 'aestival',\n",
              " 'aetiologic',\n",
              " 'aetiological',\n",
              " 'afeard',\n",
              " 'afeared',\n",
              " 'afebrile',\n",
              " 'affable',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affectional',\n",
              " 'affectionate',\n",
              " 'affective',\n",
              " 'afferent',\n",
              " 'affiliated',\n",
              " 'affinal',\n",
              " 'affine',\n",
              " 'affined',\n",
              " 'affirmable',\n",
              " 'affirmative',\n",
              " 'affirmatory',\n",
              " 'affixal',\n",
              " 'affixed',\n",
              " 'affixial',\n",
              " 'afflicted',\n",
              " 'afflictive',\n",
              " 'affluent',\n",
              " 'affordable',\n",
              " 'afghan',\n",
              " 'afghani',\n",
              " 'afghanistani',\n",
              " 'afire',\n",
              " 'aflame',\n",
              " 'aflare',\n",
              " 'aflicker',\n",
              " 'afloat',\n",
              " 'aflutter',\n",
              " 'afoot',\n",
              " 'aforementioned',\n",
              " 'aforesaid',\n",
              " 'aforethought',\n",
              " 'afoul',\n",
              " 'afraid',\n",
              " 'african',\n",
              " 'african-american',\n",
              " 'afrikaans',\n",
              " 'afrikaner',\n",
              " 'afro-american',\n",
              " 'afro-asian',\n",
              " 'aft',\n",
              " 'after',\n",
              " 'after-hours',\n",
              " 'after-school',\n",
              " 'aftermost',\n",
              " 'aftershafted',\n",
              " 'agamic',\n",
              " 'agamogenetic',\n",
              " 'agamous',\n",
              " 'agape',\n",
              " 'agaze',\n",
              " 'age-old',\n",
              " 'age-related',\n",
              " 'aged',\n",
              " 'ageing',\n",
              " 'ageless',\n",
              " 'agelong',\n",
              " 'agential',\n",
              " 'agglomerate',\n",
              " 'agglomerated',\n",
              " 'agglomerative',\n",
              " 'agglutinate',\n",
              " 'agglutinative',\n",
              " 'aggravated',\n",
              " 'aggravating',\n",
              " 'aggregate',\n",
              " 'aggregated',\n",
              " 'aggregative',\n",
              " 'aggressive',\n",
              " 'aghast',\n",
              " 'agile',\n",
              " 'aging',\n",
              " 'agitated',\n",
              " 'agitating',\n",
              " 'agitative',\n",
              " 'agleam',\n",
              " 'aglitter',\n",
              " 'aglow',\n",
              " 'agnate',\n",
              " 'agnatic',\n",
              " 'agnostic',\n",
              " 'agnostical',\n",
              " 'ago',\n",
              " 'agog',\n",
              " 'agonadal',\n",
              " 'agonal',\n",
              " 'agone',\n",
              " 'agonised',\n",
              " 'agonising',\n",
              " 'agonistic',\n",
              " 'agonistical',\n",
              " 'agonized',\n",
              " 'agonizing',\n",
              " 'agoraphobic',\n",
              " 'agranulocytic',\n",
              " 'agraphic',\n",
              " 'agrarian',\n",
              " 'agreeable',\n",
              " 'agreed',\n",
              " 'agreed_upon',\n",
              " 'agrestic',\n",
              " 'agricultural',\n",
              " 'agrobiologic',\n",
              " 'agrobiological',\n",
              " 'agrologic',\n",
              " 'agrological',\n",
              " 'agronomic',\n",
              " 'agronomical',\n",
              " 'aground',\n",
              " 'agrypnotic',\n",
              " 'aguish',\n",
              " 'ahead',\n",
              " 'ahistorical',\n",
              " 'ahorse',\n",
              " 'ahorseback',\n",
              " 'aided',\n",
              " 'ailing',\n",
              " 'aimless',\n",
              " 'ain',\n",
              " 'air-breathing',\n",
              " 'air-conditioned',\n",
              " 'air-cooled',\n",
              " 'air-dried',\n",
              " 'air-dry',\n",
              " 'air-filled',\n",
              " 'air-tight',\n",
              " 'air-to-air',\n",
              " 'air-to-ground',\n",
              " 'air-to-surface',\n",
              " 'air_sick',\n",
              " 'airborne',\n",
              " 'aired',\n",
              " 'airheaded',\n",
              " 'airless',\n",
              " 'airlike',\n",
              " 'airsick',\n",
              " 'airtight',\n",
              " 'airworthy',\n",
              " 'airy',\n",
              " 'ajar',\n",
              " 'akimbo',\n",
              " 'akin',\n",
              " 'al_dente',\n",
              " 'alabaster',\n",
              " 'alabastrine',\n",
              " 'alacritous',\n",
              " 'alar',\n",
              " 'alarmed',\n",
              " 'alarming',\n",
              " 'alary',\n",
              " 'alaskan',\n",
              " 'alate',\n",
              " 'alated',\n",
              " 'albanian',\n",
              " 'albescent',\n",
              " 'albigensian',\n",
              " 'albinal',\n",
              " 'albinic',\n",
              " 'albinistic',\n",
              " 'albinotic',\n",
              " 'albitic',\n",
              " 'albuminous',\n",
              " 'albuminuric',\n",
              " 'alcalescent',\n",
              " 'alchemic',\n",
              " 'alchemical',\n",
              " 'alchemistic',\n",
              " 'alchemistical',\n",
              " 'alcohol-dependent',\n",
              " 'alcohol-soluble',\n",
              " 'alcoholic',\n",
              " 'aldehydic',\n",
              " 'aldermanic',\n",
              " 'aldermanly',\n",
              " 'aleatory',\n",
              " 'alert',\n",
              " 'aleuronic',\n",
              " 'aleutian',\n",
              " 'alexandrian',\n",
              " 'alexic',\n",
              " 'alfresco',\n",
              " 'algal',\n",
              " 'algebraic',\n",
              " 'algebraical',\n",
              " 'algerian',\n",
              " 'algid',\n",
              " 'algoid',\n",
              " 'algolagnic',\n",
              " 'algometric',\n",
              " 'algometrical',\n",
              " 'algonkian',\n",
              " 'algonquian',\n",
              " 'algonquin',\n",
              " 'algophobic',\n",
              " 'algorithmic',\n",
              " 'alien',\n",
              " 'alienable',\n",
              " 'alienated',\n",
              " 'alienating',\n",
              " 'aliform',\n",
              " 'alight',\n",
              " 'aligned',\n",
              " 'aligning',\n",
              " 'alike',\n",
              " 'alimental',\n",
              " 'alimentary',\n",
              " 'alimentative',\n",
              " 'aliphatic',\n",
              " 'aliquot',\n",
              " 'alive',\n",
              " 'alkahestic',\n",
              " 'alkalescent',\n",
              " 'alkalic',\n",
              " 'alkaline',\n",
              " 'alkaline-loving',\n",
              " 'alkaloidal',\n",
              " 'alkalotic',\n",
              " 'alkylic',\n",
              " 'all',\n",
              " 'all-around',\n",
              " 'all-devouring',\n",
              " 'all-embracing',\n",
              " 'all-encompassing',\n",
              " 'all-fired',\n",
              " 'all-important',\n",
              " 'all-inclusive',\n",
              " 'all-knowing',\n",
              " 'all-mains',\n",
              " 'all-metal',\n",
              " 'all-night',\n",
              " 'all-or-none',\n",
              " 'all-or-nothing',\n",
              " 'all-out',\n",
              " 'all-powerful',\n",
              " 'all-purpose',\n",
              " 'all-round',\n",
              " 'all-time',\n",
              " 'all-victorious',\n",
              " 'all-weather',\n",
              " 'all_important',\n",
              " 'all_in',\n",
              " 'all_over',\n",
              " 'all_right',\n",
              " 'allantoic',\n",
              " 'allantoid',\n",
              " 'allargando',\n",
              " 'alleged',\n",
              " 'allegiant',\n",
              " 'allegoric',\n",
              " 'allegorical',\n",
              " 'allegretto',\n",
              " 'allegro',\n",
              " 'allelic',\n",
              " 'allelomorphic',\n",
              " 'allergenic',\n",
              " 'allergic',\n",
              " 'alleviated',\n",
              " 'alleviative',\n",
              " 'alleviatory',\n",
              " 'alliaceous',\n",
              " 'allied',\n",
              " 'alligatored',\n",
              " 'alliterative',\n",
              " 'allocable',\n",
              " 'allocatable',\n",
              " 'allochronic',\n",
              " 'allochthonous',\n",
              " 'allogamous',\n",
              " 'allogeneic',\n",
              " 'allographic',\n",
              " 'allomerous',\n",
              " 'allometric',\n",
              " 'allomorphic',\n",
              " 'allopathic',\n",
              " 'allopatric',\n",
              " 'allophonic',\n",
              " 'allotropic',\n",
              " 'allotropical',\n",
              " 'allotted',\n",
              " 'allover',\n",
              " 'allowable',\n",
              " 'alloyed',\n",
              " 'alluring',\n",
              " 'allusive',\n",
              " 'alluvial',\n",
              " 'allylic',\n",
              " 'almighty',\n",
              " 'almond-eyed',\n",
              " 'almond-scented',\n",
              " 'almond-shaped',\n",
              " 'alone',\n",
              " 'aloof',\n",
              " 'alopecic',\n",
              " 'alpestrine',\n",
              " 'alpha',\n",
              " 'alphabetic',\n",
              " 'alphabetical',\n",
              " 'alphabetised',\n",
              " 'alphabetized',\n",
              " 'alphameric',\n",
              " 'alphamerical',\n",
              " 'alphanumeric',\n",
              " 'alphanumerical',\n",
              " 'alpine',\n",
              " 'alright',\n",
              " 'alsatian',\n",
              " 'altaic',\n",
              " 'alterable',\n",
              " 'alterative',\n",
              " 'altered',\n",
              " 'alternate',\n",
              " 'alternating',\n",
              " 'alternative',\n",
              " 'altissimo',\n",
              " 'altitudinal',\n",
              " 'altitudinous',\n",
              " 'alto',\n",
              " 'altricial',\n",
              " 'altruistic',\n",
              " 'alular',\n",
              " 'aluminiferous',\n",
              " 'aluminous',\n",
              " 'alveolar',\n",
              " 'alveolate',\n",
              " 'alvine',\n",
              " 'amalgamate',\n",
              " 'amalgamated',\n",
              " 'amalgamative',\n",
              " 'amaranthine',\n",
              " 'amateur',\n",
              " 'amateurish',\n",
              " 'amative',\n",
              " 'amatory',\n",
              " 'amaurotic',\n",
              " 'amazed',\n",
              " 'amazing',\n",
              " 'ambagious',\n",
              " 'ambassadorial',\n",
              " 'amber',\n",
              " 'amber-green',\n",
              " 'ambidextrous',\n",
              " 'ambient',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "vocab = words.copy()\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "7EypwQxgx7rJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape, vocab_size, embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dPZi-232DZT",
        "outputId": "385787ed-757e-4798-c48a-7dcf2c69fc6f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([147306, 100]), 147306, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myembeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(myembeddings, self).__init__()\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target_indices):\n",
        "        embeddings = self.embedding_matrix[target_indices]\n",
        "        output = self.output_layer(embeddings)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "TgkN334ZxpuT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(corpus, word_to_index, context_size=2):\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "        sentence_indices = [word_to_index[word] for word in sentence if word in word_to_index]\n",
        "        for i, target in enumerate(sentence_indices):\n",
        "            context = [sentence_indices[i + j] for j in range(-context_size, context_size + 1) if j != 0 and 0 <= i + j < len(sentence_indices)]\n",
        "            for context_word in context:\n",
        "                training_data.append((target, context_word))\n",
        "    return training_data\n",
        "\n",
        "corpus = [\n",
        "    ['My', 'name', 'is','Ayush'],\n",
        "    ['Sharma', 'vivo', 'acquantance'],\n",
        "    ['the', 'cat', 'sat', 'on', 'the', 'mat.'],\n",
        "    ['a', 'dog', 'barked', 'loudly', 'in', 'the', 'park.'],\n",
        "    ['she', 'enjoys', 'reading', 'books', 'on', 'sunny', 'days.'],\n",
        "    ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.'],\n",
        "    ['he', 'writes', 'code', 'in', 'python', 'every', 'day.'],\n",
        "    ['they', 'are', 'planning', 'a', 'trip', 'to', 'the', 'mountains.'],\n",
        "    ['birds', 'are', 'singing', 'in', 'the', 'morning', 'sky.'],\n",
        "    ['the', 'chef', 'cooked', 'a', 'delicious', 'meal', 'for', 'dinner.'],\n",
        "    ['children', 'are', 'playing', 'in', 'the', 'playground.']\n",
        "]\n",
        "\n",
        "training_data = generate_training_data(corpus, word_to_index)\n"
      ],
      "metadata": {
        "id": "l_ctejEExpxM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,j in (training_data[:10]):\n",
        "#     print((index_to_word[i],index_to_word[j]))\n",
        "\n",
        "training_data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIaHGOoU3joT",
        "outputId": "b4a1ce9d-b87b-448d-d7e0-b143577f92c5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(42248, 116086),\n",
              " (42248, 12775),\n",
              " (116086, 42248),\n",
              " (116086, 12775),\n",
              " (12775, 42248)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convergence_plot=[]"
      ],
      "metadata": {
        "id": "qKXgAsTc7Zni"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDsGCiIPAYu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_data, epochs=1000, learning_rate=0.03):\n",
        "    optimizer = optim.SGD([embedding_matrix] + list(model.parameters()), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for target, context in training_data:\n",
        "            target_tensor = torch.tensor([target], dtype=torch.long)\n",
        "            context_tensor = torch.tensor([context], dtype=torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(target_tensor)\n",
        "            loss = loss_function(output, context_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss:.4f}')\n",
        "        convergence_plot.append(total_loss)\n",
        "\n",
        "\n",
        "model = myembeddings(vocab_size, embedding_dim)\n",
        "\n",
        "train_model(model, training_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbsTbRwVxpzY",
        "outputId": "20083e80-ac20-4a35-bfff-f2fcb411b67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1045.1455\n",
            "Epoch 2, Loss: 835.7778\n",
            "Epoch 3, Loss: 625.7438\n",
            "Epoch 4, Loss: 452.2658\n",
            "Epoch 5, Loss: 327.8895\n",
            "Epoch 6, Loss: 256.7898\n",
            "Epoch 7, Loss: 227.5696\n",
            "Epoch 8, Loss: 217.6617\n",
            "Epoch 9, Loss: 213.6895\n",
            "Epoch 10, Loss: 211.7053\n",
            "Epoch 11, Loss: 210.4903\n",
            "Epoch 12, Loss: 209.6389\n",
            "Epoch 13, Loss: 208.9879\n",
            "Epoch 14, Loss: 208.4548\n",
            "Epoch 15, Loss: 207.9937\n",
            "Epoch 16, Loss: 207.5786\n",
            "Epoch 17, Loss: 207.1937\n",
            "Epoch 18, Loss: 206.8295\n",
            "Epoch 19, Loss: 206.4796\n",
            "Epoch 20, Loss: 206.1400\n",
            "Epoch 21, Loss: 205.8079\n",
            "Epoch 22, Loss: 205.4815\n",
            "Epoch 23, Loss: 205.1593\n",
            "Epoch 24, Loss: 204.8406\n",
            "Epoch 25, Loss: 204.5245\n",
            "Epoch 26, Loss: 204.2107\n",
            "Epoch 27, Loss: 203.8988\n",
            "Epoch 28, Loss: 203.5887\n",
            "Epoch 29, Loss: 203.2800\n",
            "Epoch 30, Loss: 202.9728\n",
            "Epoch 31, Loss: 202.6669\n",
            "Epoch 32, Loss: 202.3623\n",
            "Epoch 33, Loss: 202.0589\n",
            "Epoch 34, Loss: 201.7568\n",
            "Epoch 35, Loss: 201.4560\n",
            "Epoch 36, Loss: 201.1563\n",
            "Epoch 37, Loss: 200.8579\n",
            "Epoch 38, Loss: 200.5608\n",
            "Epoch 39, Loss: 200.2650\n",
            "Epoch 40, Loss: 199.9705\n",
            "Epoch 41, Loss: 199.6772\n",
            "Epoch 42, Loss: 199.3853\n",
            "Epoch 43, Loss: 199.0947\n",
            "Epoch 44, Loss: 198.8055\n",
            "Epoch 45, Loss: 198.5176\n",
            "Epoch 46, Loss: 198.2312\n",
            "Epoch 47, Loss: 197.9461\n",
            "Epoch 48, Loss: 197.6625\n",
            "Epoch 49, Loss: 197.3803\n",
            "Epoch 50, Loss: 197.0995\n",
            "Epoch 51, Loss: 196.8201\n",
            "Epoch 52, Loss: 196.5422\n",
            "Epoch 53, Loss: 196.2658\n",
            "Epoch 54, Loss: 195.9908\n",
            "Epoch 55, Loss: 195.7173\n",
            "Epoch 56, Loss: 195.4452\n",
            "Epoch 57, Loss: 195.1747\n",
            "Epoch 58, Loss: 194.9055\n",
            "Epoch 59, Loss: 194.6379\n",
            "Epoch 60, Loss: 194.3717\n",
            "Epoch 61, Loss: 194.1071\n",
            "Epoch 62, Loss: 193.8439\n",
            "Epoch 63, Loss: 193.5822\n",
            "Epoch 64, Loss: 193.3219\n",
            "Epoch 65, Loss: 193.0632\n",
            "Epoch 66, Loss: 192.8059\n",
            "Epoch 67, Loss: 192.5500\n",
            "Epoch 68, Loss: 192.2956\n",
            "Epoch 69, Loss: 192.0427\n",
            "Epoch 70, Loss: 191.7913\n",
            "Epoch 71, Loss: 191.5412\n",
            "Epoch 72, Loss: 191.2926\n",
            "Epoch 73, Loss: 191.0455\n",
            "Epoch 74, Loss: 190.7998\n",
            "Epoch 75, Loss: 190.5555\n",
            "Epoch 76, Loss: 190.3127\n",
            "Epoch 77, Loss: 190.0712\n",
            "Epoch 78, Loss: 189.8312\n",
            "Epoch 79, Loss: 189.5925\n",
            "Epoch 80, Loss: 189.3553\n",
            "Epoch 81, Loss: 189.1194\n",
            "Epoch 82, Loss: 188.8849\n",
            "Epoch 83, Loss: 188.6518\n",
            "Epoch 84, Loss: 188.4200\n",
            "Epoch 85, Loss: 188.1896\n",
            "Epoch 86, Loss: 187.9606\n",
            "Epoch 87, Loss: 187.7329\n",
            "Epoch 88, Loss: 187.5065\n",
            "Epoch 89, Loss: 187.2814\n",
            "Epoch 90, Loss: 187.0576\n",
            "Epoch 91, Loss: 186.8352\n",
            "Epoch 92, Loss: 186.6140\n",
            "Epoch 93, Loss: 186.3942\n",
            "Epoch 94, Loss: 186.1756\n",
            "Epoch 95, Loss: 185.9582\n",
            "Epoch 96, Loss: 185.7422\n",
            "Epoch 97, Loss: 185.5274\n",
            "Epoch 98, Loss: 185.3139\n",
            "Epoch 99, Loss: 185.1016\n",
            "Epoch 100, Loss: 184.8905\n",
            "Epoch 101, Loss: 184.6806\n",
            "Epoch 102, Loss: 184.4720\n",
            "Epoch 103, Loss: 184.2646\n",
            "Epoch 104, Loss: 184.0583\n",
            "Epoch 105, Loss: 183.8533\n",
            "Epoch 106, Loss: 183.6494\n",
            "Epoch 107, Loss: 183.4467\n",
            "Epoch 108, Loss: 183.2452\n",
            "Epoch 109, Loss: 183.0448\n",
            "Epoch 110, Loss: 182.8456\n",
            "Epoch 111, Loss: 182.6475\n",
            "Epoch 112, Loss: 182.4505\n",
            "Epoch 113, Loss: 182.2547\n",
            "Epoch 114, Loss: 182.0600\n",
            "Epoch 115, Loss: 181.8664\n",
            "Epoch 116, Loss: 181.6739\n",
            "Epoch 117, Loss: 181.4825\n",
            "Epoch 118, Loss: 181.2922\n",
            "Epoch 119, Loss: 181.1029\n",
            "Epoch 120, Loss: 180.9147\n",
            "Epoch 121, Loss: 180.7276\n",
            "Epoch 122, Loss: 180.5415\n",
            "Epoch 123, Loss: 180.3565\n",
            "Epoch 124, Loss: 180.1725\n",
            "Epoch 125, Loss: 179.9896\n",
            "Epoch 126, Loss: 179.8076\n",
            "Epoch 127, Loss: 179.6267\n",
            "Epoch 128, Loss: 179.4468\n",
            "Epoch 129, Loss: 179.2679\n",
            "Epoch 130, Loss: 179.0900\n",
            "Epoch 131, Loss: 178.9131\n",
            "Epoch 132, Loss: 178.7371\n",
            "Epoch 133, Loss: 178.5621\n",
            "Epoch 134, Loss: 178.3881\n",
            "Epoch 135, Loss: 178.2151\n",
            "Epoch 136, Loss: 178.0430\n",
            "Epoch 137, Loss: 177.8718\n",
            "Epoch 138, Loss: 177.7016\n",
            "Epoch 139, Loss: 177.5324\n",
            "Epoch 140, Loss: 177.3640\n",
            "Epoch 141, Loss: 177.1966\n",
            "Epoch 142, Loss: 177.0300\n",
            "Epoch 143, Loss: 176.8644\n",
            "Epoch 144, Loss: 176.6997\n",
            "Epoch 145, Loss: 176.5359\n",
            "Epoch 146, Loss: 176.3729\n",
            "Epoch 147, Loss: 176.2109\n",
            "Epoch 148, Loss: 176.0497\n",
            "Epoch 149, Loss: 175.8894\n",
            "Epoch 150, Loss: 175.7299\n",
            "Epoch 151, Loss: 175.5713\n",
            "Epoch 152, Loss: 175.4135\n",
            "Epoch 153, Loss: 175.2566\n",
            "Epoch 154, Loss: 175.1005\n",
            "Epoch 155, Loss: 174.9453\n",
            "Epoch 156, Loss: 174.7909\n",
            "Epoch 157, Loss: 174.6373\n",
            "Epoch 158, Loss: 174.4845\n",
            "Epoch 159, Loss: 174.3325\n",
            "Epoch 160, Loss: 174.1814\n",
            "Epoch 161, Loss: 174.0310\n",
            "Epoch 162, Loss: 173.8814\n",
            "Epoch 163, Loss: 173.7326\n",
            "Epoch 164, Loss: 173.5846\n",
            "Epoch 165, Loss: 173.4373\n",
            "Epoch 166, Loss: 173.2908\n",
            "Epoch 167, Loss: 173.1451\n",
            "Epoch 168, Loss: 173.0002\n",
            "Epoch 169, Loss: 172.8560\n",
            "Epoch 170, Loss: 172.7125\n",
            "Epoch 171, Loss: 172.5698\n",
            "Epoch 172, Loss: 172.4278\n",
            "Epoch 173, Loss: 172.2866\n",
            "Epoch 174, Loss: 172.1461\n",
            "Epoch 175, Loss: 172.0063\n",
            "Epoch 176, Loss: 171.8672\n",
            "Epoch 177, Loss: 171.7289\n",
            "Epoch 178, Loss: 171.5912\n",
            "Epoch 179, Loss: 171.4543\n",
            "Epoch 180, Loss: 171.3180\n",
            "Epoch 181, Loss: 171.1825\n",
            "Epoch 182, Loss: 171.0476\n",
            "Epoch 183, Loss: 170.9134\n",
            "Epoch 184, Loss: 170.7799\n",
            "Epoch 185, Loss: 170.6471\n",
            "Epoch 186, Loss: 170.5149\n",
            "Epoch 187, Loss: 170.3834\n",
            "Epoch 188, Loss: 170.2526\n",
            "Epoch 189, Loss: 170.1224\n",
            "Epoch 190, Loss: 169.9929\n",
            "Epoch 191, Loss: 169.8641\n",
            "Epoch 192, Loss: 169.7358\n",
            "Epoch 193, Loss: 169.6082\n",
            "Epoch 194, Loss: 169.4813\n",
            "Epoch 195, Loss: 169.3550\n",
            "Epoch 196, Loss: 169.2293\n",
            "Epoch 197, Loss: 169.1042\n",
            "Epoch 198, Loss: 168.9798\n",
            "Epoch 199, Loss: 168.8559\n",
            "Epoch 200, Loss: 168.7327\n",
            "Epoch 201, Loss: 168.6101\n",
            "Epoch 202, Loss: 168.4881\n",
            "Epoch 203, Loss: 168.3667\n",
            "Epoch 204, Loss: 168.2458\n",
            "Epoch 205, Loss: 168.1256\n",
            "Epoch 206, Loss: 168.0059\n",
            "Epoch 207, Loss: 167.8869\n",
            "Epoch 208, Loss: 167.7684\n",
            "Epoch 209, Loss: 167.6505\n",
            "Epoch 210, Loss: 167.5332\n",
            "Epoch 211, Loss: 167.4164\n",
            "Epoch 212, Loss: 167.3002\n",
            "Epoch 213, Loss: 167.1846\n",
            "Epoch 214, Loss: 167.0695\n",
            "Epoch 215, Loss: 166.9550\n",
            "Epoch 216, Loss: 166.8410\n",
            "Epoch 217, Loss: 166.7275\n",
            "Epoch 218, Loss: 166.6146\n",
            "Epoch 219, Loss: 166.5023\n",
            "Epoch 220, Loss: 166.3905\n",
            "Epoch 221, Loss: 166.2792\n",
            "Epoch 222, Loss: 166.1685\n",
            "Epoch 223, Loss: 166.0582\n",
            "Epoch 224, Loss: 165.9485\n",
            "Epoch 225, Loss: 165.8394\n",
            "Epoch 226, Loss: 165.7307\n",
            "Epoch 227, Loss: 165.6226\n",
            "Epoch 228, Loss: 165.5149\n",
            "Epoch 229, Loss: 165.4078\n",
            "Epoch 230, Loss: 165.3012\n",
            "Epoch 231, Loss: 165.1950\n",
            "Epoch 232, Loss: 165.0894\n",
            "Epoch 233, Loss: 164.9842\n",
            "Epoch 234, Loss: 164.8796\n",
            "Epoch 235, Loss: 164.7754\n",
            "Epoch 236, Loss: 164.6718\n",
            "Epoch 237, Loss: 164.5686\n",
            "Epoch 238, Loss: 164.4659\n",
            "Epoch 239, Loss: 164.3636\n",
            "Epoch 240, Loss: 164.2619\n",
            "Epoch 241, Loss: 164.1606\n",
            "Epoch 242, Loss: 164.0598\n",
            "Epoch 243, Loss: 163.9594\n",
            "Epoch 244, Loss: 163.8595\n",
            "Epoch 245, Loss: 163.7601\n",
            "Epoch 246, Loss: 163.6611\n",
            "Epoch 247, Loss: 163.5625\n",
            "Epoch 248, Loss: 163.4645\n",
            "Epoch 249, Loss: 163.3668\n",
            "Epoch 250, Loss: 163.2696\n",
            "Epoch 251, Loss: 163.1729\n",
            "Epoch 252, Loss: 163.0766\n",
            "Epoch 253, Loss: 162.9807\n",
            "Epoch 254, Loss: 162.8853\n",
            "Epoch 255, Loss: 162.7903\n",
            "Epoch 256, Loss: 162.6957\n",
            "Epoch 257, Loss: 162.6016\n",
            "Epoch 258, Loss: 162.5079\n",
            "Epoch 259, Loss: 162.4146\n",
            "Epoch 260, Loss: 162.3217\n",
            "Epoch 261, Loss: 162.2292\n",
            "Epoch 262, Loss: 162.1372\n",
            "Epoch 263, Loss: 162.0455\n",
            "Epoch 264, Loss: 161.9543\n",
            "Epoch 265, Loss: 161.8635\n",
            "Epoch 266, Loss: 161.7732\n",
            "Epoch 267, Loss: 161.6831\n",
            "Epoch 268, Loss: 161.5935\n",
            "Epoch 269, Loss: 161.5043\n",
            "Epoch 270, Loss: 161.4155\n",
            "Epoch 271, Loss: 161.3271\n",
            "Epoch 272, Loss: 161.2390\n",
            "Epoch 273, Loss: 161.1514\n",
            "Epoch 274, Loss: 161.0641\n",
            "Epoch 275, Loss: 160.9772\n",
            "Epoch 276, Loss: 160.8907\n",
            "Epoch 277, Loss: 160.8046\n",
            "Epoch 278, Loss: 160.7189\n",
            "Epoch 279, Loss: 160.6335\n",
            "Epoch 280, Loss: 160.5485\n",
            "Epoch 281, Loss: 160.4639\n",
            "Epoch 282, Loss: 160.3796\n",
            "Epoch 283, Loss: 160.2958\n",
            "Epoch 284, Loss: 160.2122\n",
            "Epoch 285, Loss: 160.1290\n",
            "Epoch 286, Loss: 160.0462\n",
            "Epoch 287, Loss: 159.9638\n",
            "Epoch 288, Loss: 159.8817\n",
            "Epoch 289, Loss: 159.8000\n",
            "Epoch 290, Loss: 159.7186\n",
            "Epoch 291, Loss: 159.6375\n",
            "Epoch 292, Loss: 159.5569\n",
            "Epoch 293, Loss: 159.4765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(f=\"model.pth\", obj=model)"
      ],
      "metadata": {
        "id": "ywKMMvEJRFs6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trained_embedding_matrix = model.embedding_matrix.detach()\n"
      ],
      "metadata": {
        "id": "NMxV64uRxp12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_embedding_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2gi6am_046g",
        "outputId": "56063dfa-24c3-485e-9560-d1504c256ebf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5389,  0.9008,  2.0919, -0.2412, -0.4483, -2.7679,  0.0570, -0.7687,\n",
              "         -0.5733, -0.0213],\n",
              "        [ 0.3102, -0.7673, -0.5406, -1.5411, -0.2066, -0.0885,  0.8541, -0.7215,\n",
              "          0.0326,  0.1048],\n",
              "        [ 0.4902,  0.2057,  0.6303, -1.2802, -1.0341, -1.2794, -0.8239, -1.1425,\n",
              "          0.3443,  1.7529],\n",
              "        [-0.6403, -0.2694, -0.4505,  1.6121, -0.6620, -1.4094, -1.1139, -0.3102,\n",
              "         -0.0135,  0.7385],\n",
              "        [ 0.6618,  1.7445, -1.0322,  0.0384, -1.3838,  0.4387, -1.0931,  1.4958,\n",
              "          0.9368,  0.1841]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "embedding_layer.weight = nn.Parameter(trained_embedding_matrix)\n"
      ],
      "metadata": {
        "id": "8_qqPcVBxp4D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUaQgai90yIi",
        "outputId": "9cd6ad2f-102d-46bd-e175-e6f353136300"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.5389,  0.9008,  2.0919, -0.2412, -0.4483, -2.7679,  0.0570, -0.7687,\n",
              "         -0.5733, -0.0213],\n",
              "        [ 0.3102, -0.7673, -0.5406, -1.5411, -0.2066, -0.0885,  0.8541, -0.7215,\n",
              "          0.0326,  0.1048],\n",
              "        [ 0.4902,  0.2057,  0.6303, -1.2802, -1.0341, -1.2794, -0.8239, -1.1425,\n",
              "          0.3443,  1.7529],\n",
              "        [-0.6403, -0.2694, -0.4505,  1.6121, -0.6620, -1.4094, -1.1139, -0.3102,\n",
              "         -0.0135,  0.7385],\n",
              "        [ 0.6618,  1.7445, -1.0322,  0.0384, -1.3838,  0.4387, -1.0931,  1.4958,\n",
              "          0.9368,  0.1841]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDqL3xNixp6o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_vector(word, embedding_layer, word_to_index):\n",
        "    word_index = word_to_index.get(word, None)\n",
        "\n",
        "    if word_index is None:\n",
        "        raise ValueError(f\"The word '{word}' is not in the vocabulary.\")\n",
        "    word_index_tensor = torch.tensor([word_index], dtype=torch.long)\n",
        "    word_vector = embedding_layer(word_index_tensor)\n",
        "\n",
        "    return word_vector.squeeze(0)\n"
      ],
      "metadata": {
        "id": "eLr8Jwxgxp-N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "vector = word_to_vector(word, embedding_layer, word_to_index)\n",
        "\n",
        "print(f\"Vector for '{word}':\\n{vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2akmrX-B1e64",
        "outputId": "a25821bb-6e3e-44cc-bfb7-4775b2b0d444"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'word5':\n",
            "tensor([ 0.6618,  1.7445, -1.0322,  0.0384, -1.3838,  0.4387, -1.0931,  1.4958,\n",
            "         0.9368,  0.1841], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    }
  ]
}